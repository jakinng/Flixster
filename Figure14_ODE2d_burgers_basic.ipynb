{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Z7AAWXrzFewZhX_yQAbbG-WGPVYhprnq",
      "authorship_tag": "ABX9TyOF0T0lMrwexl7Gr1rtlnY5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xueenwu/Flixster/blob/master/Figure14_ODE2d_burgers_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install pyDOE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBehqVZNLI7B",
        "outputId": "ed812e96-a5fd-4999-ee2c-92e58574da9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyDOE in /usr/local/lib/python3.10/dist-packages (0.3.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.11.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import\n",
        "\n",
        "import sys\n",
        "import jax.config as config\n",
        "import jax.numpy as jnp\n",
        "# import numpy as np\n",
        "import optax\n",
        "from jax import random, jit, vjp, grad, vmap, pmap\n",
        "import jax.flatten_util as flat_utl\n",
        "from jax.experimental.host_callback import call\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "from pyDOE import lhs\n",
        "import time\n",
        "import functools\n",
        "import scipy.io\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "CM31G_ZSMChh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EJAxgTTpLA1B"
      },
      "outputs": [],
      "source": [
        "#@title Neural Net Functions\n",
        "\n",
        "# initialize the neural network weights and biases\n",
        "def init_MLP(parent_key, layer_widths):\n",
        "    params = []\n",
        "    keys = random.split(parent_key, num=len(layer_widths) - 1)\n",
        "    # create the weights and biases for the network\n",
        "    for in_dim, out_dim, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "        weight_key, bias_key = random.split(key)\n",
        "        xavier_stddev = jnp.sqrt(2 / (in_dim + out_dim))\n",
        "        params.append(\n",
        "            [random.truncated_normal(weight_key, -2, 2, shape=(in_dim, out_dim)) * xavier_stddev,\n",
        "             random.truncated_normal(bias_key, -2, 2, shape=(out_dim,)) * xavier_stddev]\n",
        "        )\n",
        "    return params\n",
        "\n",
        "\n",
        "# define the basic formation of neural network\n",
        "def neural_net(params, z, limit, scl, act_s):\n",
        "    '''\n",
        "    :param params: weights and biases\n",
        "    :param z: input data [matrix with shape [N, m]]; m is number of inputs)\n",
        "    :param limit: characteristic scale for normalization [matrix with shape [2, m]]\n",
        "    :param scl: scale factor to scale gradient\n",
        "    :param act_s: activation function for the first layer\n",
        "    :return: neural network output [matrix with shape [N, n]]; n is number of outputs)\n",
        "    '''\n",
        "    lb = limit[0]  # lower bound for each input\n",
        "    ub = limit[1]  # upper bound for each input\n",
        "\n",
        "    # choose the activation function\n",
        "    actv = [jnp.tanh, jnp.sin][act_s]\n",
        "    # normalize the input\n",
        "    H = 2.0 * (z - lb) / (ub - lb) - 1.0\n",
        "    # separate the first, hidden and last layers\n",
        "    first, *hidden, last = params\n",
        "    # calculate the first layers output with right scale\n",
        "    H = actv(jnp.dot(H, first[0]) * scl + first[1])\n",
        "    # calculate the middle layers output\n",
        "    for layer in hidden:\n",
        "        H = jnp.tanh(jnp.dot(H, layer[0]) + layer[1])\n",
        "    # no activation function for last layer\n",
        "    var = jnp.dot(H, last[0]) + last[1]\n",
        "    return var\n",
        "\n",
        "\n",
        "# define sech function\n",
        "def sech(z):\n",
        "    return 1 / jnp.cosh(z)\n",
        "\n",
        "\n",
        "# generate weights and biases for all variables of CLM problem\n",
        "def sol_init_MLP(parent_key, n_hl, n_unit):\n",
        "    '''\n",
        "    :param n_hl: number of hidden layers [int]\n",
        "    :param n_unit: number of units in each layer [int]\n",
        "    '''\n",
        "    layers = [2] + n_hl * [n_unit] + [1]\n",
        "    # generate the random key for each network\n",
        "    keys = random.split(parent_key, 1)\n",
        "    # generate weights and biases for\n",
        "    params_u = init_MLP(keys[0], layers)\n",
        "    return dict(net_u=params_u)\n",
        "\n",
        "\n",
        "# wrapper to create solution function with given domain size\n",
        "def sol_pred_create(limit, scl, epsil, act_s=0):\n",
        "    '''\n",
        "    :param limit: domain size of the input\n",
        "    :return: function of the solution (a callable)\n",
        "    '''\n",
        "    def f_u(params, z):\n",
        "        # generate the NN\n",
        "        u = epsil * neural_net(params['net_u'], z, limit, scl, act_s)\n",
        "        return u\n",
        "    return f_u\n",
        "\n",
        "\n",
        "def mNN_pred_create(f_u, limit, scl, epsil, act_s=0):\n",
        "    '''\n",
        "    :param f_u: sum of previous stage network\n",
        "    :param limit: domain size of the input\n",
        "    :return: function of the solution (a callable)\n",
        "    '''\n",
        "    def f_comb(params, z):\n",
        "        # generate the NN\n",
        "        u_now = neural_net(params['net_u'], z, limit, scl, act_s)\n",
        "        u = f_u(z) + epsil * u_now\n",
        "        return u\n",
        "    return f_comb\n",
        "\n",
        "\n",
        "\"\"\"Low-level functions developed for PINN training using JAX\"\"\"\n",
        "\n",
        "# define the mean squared error\n",
        "def ms_error(diff):\n",
        "    return jnp.mean(jnp.square(diff), axis=0)\n",
        "\n",
        "\n",
        "# define the mean squared error with weights\n",
        "def ms_bias(diff, weight):\n",
        "    mdiff = 5 * jnp.tanh(0.2*diff) * weight\n",
        "    return jnp.mean(jnp.square(mdiff), axis=0)\n",
        "\n",
        "\n",
        "# generate matrix required for vjp for vector gradient\n",
        "def vgmat(z, n_out, idx=None):\n",
        "    '''\n",
        "    :param n_out: number of output variables\n",
        "    :param idx: indice (list) of the output variable to take the gradient\n",
        "    '''\n",
        "    if idx is None:\n",
        "        idx = range(n_out)\n",
        "    # obtain the number of index\n",
        "    n_idx = len(idx)\n",
        "    # obtain the number of input points\n",
        "    n_pt = z.shape[0]\n",
        "    # determine the shape of the gradient matrix\n",
        "    mat_shape = [n_idx, n_pt, n_out]\n",
        "    # create the zero matrix based on the shape\n",
        "    mat = jnp.zeros(mat_shape)\n",
        "    # choose the associated element in the matrix to 1\n",
        "    for l, ii in zip(range(n_idx), idx):\n",
        "        mat = mat.at[l, :, ii].set(1.)\n",
        "    return mat\n",
        "\n",
        "\n",
        "# vector gradient of the output with with input\n",
        "def vectgrad(func, z):\n",
        "    # obtain the output and the gradient function\n",
        "    sol, vjp_fn = vjp(func, z)\n",
        "    # determine the mat grad\n",
        "    mat = vgmat(z, sol.shape[1])\n",
        "    # calculate the gradient of each output with respect to each input\n",
        "    grad_sol = vmap(vjp_fn, in_axes=0)(mat)[0]\n",
        "    # calculate the total partial derivative of output with input\n",
        "    n_pd = z.shape[1] * sol.shape[1]\n",
        "    # reshape the derivative of output with input\n",
        "    grad_all = grad_sol.transpose(1, 0, 2).reshape(z.shape[0], n_pd)\n",
        "    return grad_all, sol\n",
        "\n",
        "\n",
        "# governing equation\n",
        "def gov_eqn(f_u, x, nu=1):\n",
        "    u_g, u = vectgrad(f_u, x)\n",
        "    u_t = u_g[:, 0:1]\n",
        "    u_x = u_g[:, 1:2]\n",
        "\n",
        "    fu_x = lambda x: vectgrad(f_u, x)[0][:, 1:2]\n",
        "    # calculate the output and its derivative with original coordinates\n",
        "    u_xx = vectgrad(fu_x, x)[0][:, 1:2]\n",
        "    # calculate the residue of the CCF equation\n",
        "    f = u_t + u*u_x - nu * u_xx\n",
        "    return f\n",
        "\n",
        "def gov_deri_eqn(f_u, z, nu):\n",
        "    # allocate the value to each variable\n",
        "    fc_res = lambda z: gov_eqn(f_u, z, nu)\n",
        "    # calculate the residue of higher derivative of CCF equation\n",
        "    dfunc = lambda z: vectgrad(fc_res, z)[0]\n",
        "    # calculate the residue of the first and second derivative of CCF equation\n",
        "    d2f, df = vectgrad(dfunc, z)\n",
        "    return df, d2f\n",
        "\n",
        "def loss_create(predf_u, nu, lw, loss_ref):\n",
        "    '''\n",
        "    a function factory to create the loss function based on given info\n",
        "    :param loss_ref: loss value at the initial of the training\n",
        "    :return: a loss function (callable)\n",
        "    '''\n",
        "\n",
        "    # loss function used for the PINN training\n",
        "    def loss_fun(params, data):\n",
        "        # create the function for gradient calculation involves input Z only\n",
        "        f_u = lambda z: predf_u(params, z)\n",
        "        # load the data of normalization condition\n",
        "        z_bd = data['cond_bd'][0]\n",
        "        u_bd = data['cond_bd'][1]\n",
        "\n",
        "        # load the position and weight of collocation points\n",
        "        z_col = data['z_col']\n",
        "\n",
        "        # calculate the gradient of phi at origin\n",
        "        u_bd_p = f_u(z_bd)\n",
        "\n",
        "        # calculate the residue of equation\n",
        "        f = gov_eqn(f_u, z_col, nu)\n",
        "        # calculate the residue of first and second derivative\n",
        "        # df, d2f = gov_deri_eqn(f_u, z_col)\n",
        "\n",
        "        # calculate the mean squared root error of normalization cond.\n",
        "        norm_err = ms_error(u_bd_p - u_bd)\n",
        "        # calculate the error of far-field exponent cond.\n",
        "        data_err = jnp.hstack([norm_err])\n",
        "\n",
        "        # calculate the mean squared root error of equation\n",
        "        eqn_err_f = ms_error(f)\n",
        "        # eqn_err_df = ms_error(df)\n",
        "        # eqn_err_d2f = ms_error(d2f)\n",
        "        eqn_err = jnp.hstack([eqn_err_f])\n",
        "\n",
        "        # set the weight for each condition and equation\n",
        "        data_weight = jnp.array([1.])\n",
        "        eqn_weight = jnp.array([1.])\n",
        "\n",
        "        # calculate the overall data loss and equation loss\n",
        "        loss_data = jnp.sum(data_err * data_weight)\n",
        "        loss_eqn = jnp.sum(eqn_err * eqn_weight)\n",
        "\n",
        "        # calculate the total loss\n",
        "        loss = (loss_data + lw[0] * loss_eqn) / loss_ref\n",
        "        # group the loss of all conditions and equations\n",
        "        loss_info = jnp.hstack([jnp.array([loss, loss_data, loss_eqn]),\n",
        "                                data_err, eqn_err])\n",
        "        return loss, loss_info\n",
        "    return loss_fun\n",
        "\n",
        "def loss_create_lbfgs(loss_f, weights):\n",
        "    # loss function used for the PINN training\n",
        "    def loss_fun(params, data):\n",
        "        loss, loss_info = loss_f(params, weights, data)\n",
        "        return loss, loss_info\n",
        "    return loss_fun\n",
        "\n",
        "# create the Adam minimizer\n",
        "@functools.partial(jit, static_argnames=(\"lossf\", \"opt\"))\n",
        "def adam_minimizer(lossf, params, data, opt, opt_state):\n",
        "    \"\"\"Basic gradient update step based on the opt optimizer.\"\"\"\n",
        "    grads, loss_info = grad(lossf, has_aux=True)(params, data)\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, loss_info, opt_state\n",
        "\n",
        "def adam_optimizer(lossf, params, data, epoch, lr=1e-3):\n",
        "    # select the Adam as the minimizer\n",
        "    opt_Adam = optax.adam(learning_rate=lr)\n",
        "    # obtain the initial state of the params\n",
        "    opt_state = opt_Adam.init(params)\n",
        "    # pre-allocate the loss varaible\n",
        "    loss_all = []\n",
        "    nc = jnp.int32(jnp.round(epoch / 5))\n",
        "    # start the training iteration\n",
        "    for step in range(epoch):\n",
        "        # minimize the loss function using Adam\n",
        "        params, loss_info, opt_state = adam_minimizer(lossf, params, data, opt_Adam, opt_state)\n",
        "        # print the loss for every 100 iterations\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Step: {step} | Loss: {loss_info[0]:.4e} |\"\n",
        "                  f\" Loss_d: {loss_info[1]:.4e} | Loss_e: {loss_info[2]:.4e}\", file=sys.stderr)\n",
        "        # saving the loss\n",
        "        loss_all.append(loss_info[0:3])\n",
        "\n",
        "        ## Question: what does this do?\n",
        "        if step % 10000 == 0 and step > 0:\n",
        "            lr = lr\n",
        "            opt_Adam = optax.adam(learning_rate=lr)\n",
        "\n",
        "    # obtain the total loss in the last iterations\n",
        "    lossend = jnp.array(loss_all[-nc:])[:, 0]\n",
        "    # find the minimum loss value\n",
        "    lmin = jnp.min(lossend)\n",
        "    # optain the last loss value\n",
        "    llast = lossend[-1]\n",
        "    # guarantee the loss value in last iteration is smaller than anyone before\n",
        "    while llast > lmin:\n",
        "        params, loss_info, opt_state = adam_minimizer(lossf, params, data, opt_Adam, opt_state)\n",
        "        llast = loss_info[0]\n",
        "        # saving the loss\n",
        "        loss_all.append(loss_info[0:3])\n",
        "\n",
        "    return params, loss_all\n",
        "\n",
        "# A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
        "def lbfgs_function(lossf, init_params, data):\n",
        "    # obtain the 1D parameters and the function that can turn back to the pytree\n",
        "    _, unflat = flat_utl.ravel_pytree(init_params)\n",
        "\n",
        "    def update(params_1d):\n",
        "        # updating the model's parameters from the 1D array\n",
        "        params = unflat(params_1d)\n",
        "        return params\n",
        "\n",
        "    # A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
        "    @jit\n",
        "    def f(params_1d):\n",
        "        # convert the 1d parameters back to pytree format\n",
        "        params = update(params_1d)\n",
        "        # calculate gradients and convert to 1D tf.Tensor\n",
        "        grads, loss_info = grad(lossf, has_aux=True)(params, data)\n",
        "        # convert the grad to 1d arrays\n",
        "        grads_1d = flat_utl.ravel_pytree(grads)[0]\n",
        "        loss_value = loss_info[0]\n",
        "\n",
        "        # # store loss value so we can retrieve later\n",
        "        call(lambda x: f.loss.append(x), loss_info[0:3], result_shape=None)\n",
        "        call(lambda x: print(f\"Step: NaN | Loss: {x[0]:.4e} |\"\n",
        "                  f\" Loss_d: {x[1]:.4e} | Loss_e: {x[2]:.4e}\"), loss_info)\n",
        "        return loss_value, grads_1d\n",
        "\n",
        "    # store these information as members so we can use them outside the scope\n",
        "    f.update = update\n",
        "    f.loss = []\n",
        "    return f\n",
        "\n",
        "# define the function to apply the L-BFGS optimizer\n",
        "def lbfgs_optimizer(lossf, params, data, epoch):\n",
        "    func_lbfgs = lbfgs_function(lossf, params, data)\n",
        "    # convert initial model parameters to a 1D array\n",
        "    init_params_1d = flat_utl.ravel_pytree(params)[0]\n",
        "    # calculate the effective number of iteration\n",
        "    max_nIter = jnp.int32(epoch / 3)\n",
        "    # train the model with L-BFGS solver\n",
        "    results = tfp.optimizer.lbfgs_minimize(\n",
        "        value_and_gradients_function=func_lbfgs, initial_position=init_params_1d,\n",
        "        tolerance=1e-10, max_iterations=max_nIter)\n",
        "    params = func_lbfgs.update(results.position)\n",
        "    # history = func_lbfgs.loss\n",
        "    num_iter = results.num_objective_evaluations\n",
        "    loss_all = func_lbfgs.loss\n",
        "    print(f\" Total iterations: {num_iter}\")\n",
        "    return params, loss_all\n",
        "\n",
        "def data_func_create(N_col, N_bd, ds):\n",
        "    # generate the weights for boundary and collocation points\n",
        "    w_bd = jnp.ones([N_bd*4, 1])\n",
        "    w_col = jnp.ones([N_col+N_bd*4, 1])\n",
        "    weight = dict(w_bd=w_bd, w_col=w_col)\n",
        "\n",
        "    # define the function that can re-sample collocation points each time it's called calling\n",
        "    def dataf():\n",
        "        # set the initial and boundary conditions\n",
        "        tx0 = lhs(2, N_bd) * jnp.array([1, 0]) + jnp.array([0, -1])\n",
        "        ux0 = 0 * jnp.ones(N_bd)[:, None]\n",
        "\n",
        "        tx1 = lhs(2, N_bd) * jnp.array([1, 0]) + jnp.array([0, 1])\n",
        "        ux1 = 0 * jnp.ones(N_bd)[:, None]\n",
        "\n",
        "        t0x = (2 * lhs(2, 2*N_bd) - 1) * jnp.array([0, 1])\n",
        "        ut0 = -jnp.sin(jnp.pi * t0x[:, 1:2])\n",
        "\n",
        "        # group the initial and boundary conditions\n",
        "        x_bd = jnp.vstack([tx0, tx1, t0x])\n",
        "        u_bd = jnp.vstack([ux0, ux1, ut0])\n",
        "\n",
        "        # prepare the collocation points\n",
        "        x_col = lhs(2, N_col) * jnp.array([1, 2]) - jnp.array([0, 1])\n",
        "        # add the collocation at the boundary\n",
        "        x_col = jnp.vstack([x_col, x_bd])\n",
        "\n",
        "        # group all the conditions and collocation points\n",
        "        data = dict(z_col=x_col, cond_bd=[x_bd, u_bd])\n",
        "        return data\n",
        "    return dataf, weight\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Data\n",
        "\n",
        "def init_turbulence_data(filename = '/content/drive/MyDrive/Stanford/FDNS Psi W_train_Re_1k.mat'):\n",
        "  import numpy as np\n",
        "  import h5py\n",
        "  f = h5py.File(filename,'r')\n",
        "  print(f)\n",
        "  for key in f.keys():\n",
        "      print(key) #Names of the root level object names in HDF5 file - can be groups or datasets.\n",
        "      print(type(f[key])) # get the object type: usually group or dataset\n",
        "  #Get the HDF5 dataset; key needs to be a group name from above\n",
        "  psi = np.array(f['Psi'][:])\n",
        "  omega = np.array(f['W'][:])\n",
        "  return psi, omega"
      ],
      "metadata": {
        "id": "zO5SQedUSwpH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Problem\n",
        "\n",
        "\"\"\" find the root directory \"\"\"\n",
        "# rootdir = Path(__file__).parent\n",
        "rootdir = Path(\"/content/drive/MyDrive/Stanford/1_31\")\n",
        "\n",
        "precision = jnp.float64\n",
        "# change JAX to double precision\n",
        "config.update('jax_enable_x64', True)\n",
        "\n",
        "\"\"\"Set the conditions of the problem\"\"\"\n",
        "\n",
        "# select the random seed\n",
        "seed = 1234\n",
        "key = random.PRNGKey(seed)\n",
        "# jnp.random.seed(seed)\n",
        "\n",
        "# create the subkeys\n",
        "keys = random.split(key, 4)\n",
        "\n",
        "# select the size of neural network\n",
        "n_hl = 6\n",
        "n_unit = 30\n",
        "scl = 1\n",
        "epsil = 1\n",
        "nu = 0.01\n",
        "act_s = 0\n",
        "\n",
        "# number of sampling points\n",
        "N_col = 10000\n",
        "N_bd = 100\n",
        "\n",
        "# set the size of domain\n",
        "ds = 1.\n",
        "lmt = jnp.array([[-ds], [ds]])\n",
        "\n",
        "'''\n",
        "loading the dataset and define the domain\n",
        "=============================================\n",
        "'''\n",
        "t = jnp.linspace(0, 1, 101)\n",
        "x = jnp.linspace(-1, 1, 201)\n",
        "\n",
        "T, X = jnp.meshgrid(t, x)\n",
        "X_star = jnp.hstack((T.flatten()[:, None], X.flatten()[:, None]))\n",
        "\n",
        "# Domain bounds\n",
        "lb = jnp.array([0, -0.1])  # X_star.min(0)\n",
        "ub = jnp.array([1, 0.1])   # X_star.max(0)\n",
        "limit = [lb, ub]\n",
        "\n",
        "# set the training iteration\n",
        "epoch1 = 100\n",
        "epoch2 = 100\n",
        "lw = [1e-1]\n",
        "\n",
        "# initialize the weights and biases of the network\n",
        "trained_params = sol_init_MLP(keys[0], n_hl, n_unit)\n",
        "\n",
        "# create the solution function\n",
        "pred_u = sol_pred_create(limit, scl, epsil=epsil, act_s=act_s)\n",
        "\n",
        "# create the data function\n",
        "dataf, weights = data_func_create(N_col, N_bd, ds)\n",
        "data = dataf()\n",
        "\n",
        "# calculate the loss function\n",
        "NN_loss = loss_create(pred_u, nu, lw, loss_ref=1)\n",
        "loss0 = NN_loss(trained_params, data)[0]\n",
        "NN_loss = loss_create(pred_u, nu, lw, loss_ref=loss0)\n",
        "\n",
        "\"\"\"Training using Adam\"\"\"\n",
        "\n",
        "# set the learning rate for Adam\n",
        "lr = 5e-3\n",
        "# training the neural network\n",
        "start_time = time.time()\n",
        "trained_params, loss1 = adam_optimizer(NN_loss, trained_params, data, epoch1, lr=lr)\n",
        "trained_params, loss2 = lbfgs_optimizer(NN_loss, trained_params, data, epoch2)\n",
        "\n",
        "# calculate the equation residue\n",
        "f_up = lambda z: pred_u(trained_params, z)\n",
        "\n",
        "# calculate the solution\n",
        "u_p = f_up(X_star)\n",
        "f_p = gov_eqn(f_up, X_star, nu)\n",
        "\n",
        "U = jnp.reshape(u_p, X.shape)\n",
        "F = jnp.reshape(f_p, X.shape)\n",
        "\n",
        "# generate the last loss\n",
        "n_iters = [len(loss1), len(loss2)]\n",
        "loss_all = jnp.array(loss1+loss2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6DMskWvMf1o",
        "outputId": "57a87ebf-e69d-4f2c-ae7d-1f15a418e277"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Step: 0 | Loss: 1.0000e+00 | Loss_d: 1.7460e-01 | Loss_e: 1.6879e-02\n",
            "Step: 10 | Loss: 5.5747e-01 | Loss_d: 7.8013e-02 | Loss_e: 2.0261e-01\n",
            "Step: 20 | Loss: 3.5059e-01 | Loss_d: 4.9786e-02 | Loss_e: 1.2019e-01\n",
            "Step: 30 | Loss: 2.3828e-01 | Loss_d: 1.9558e-02 | Loss_e: 2.2448e-01\n",
            "Step: 40 | Loss: 1.9096e-01 | Loss_d: 1.4055e-02 | Loss_e: 1.9609e-01\n",
            "Step: 50 | Loss: 1.6770e-01 | Loss_d: 1.3982e-02 | Loss_e: 1.5581e-01\n",
            "Step: 60 | Loss: 1.4713e-01 | Loss_d: 1.0849e-02 | Loss_e: 1.5088e-01\n",
            "Step: 70 | Loss: 1.3738e-01 | Loss_d: 1.1672e-02 | Loss_e: 1.2546e-01\n",
            "Step: 80 | Loss: 1.2638e-01 | Loss_d: 9.6583e-03 | Loss_e: 1.2621e-01\n",
            "Step: 90 | Loss: 1.1487e-01 | Loss_d: 7.5705e-03 | Loss_e: 1.2679e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: NaN | Loss: 1.0424e-01 | Loss_d: 6.7700e-03 | Loss_e: 1.1605e-01\n",
            "Step: NaN | Loss: 4.4078e+00 | Loss_d: 5.0997e-01 | Loss_e: 2.6707e+00\n",
            "Step: NaN | Loss: 4.9241e+00 | Loss_d: 7.5921e-01 | Loss_e: 1.0884e+00\n",
            "Step: NaN | Loss: 4.9024e+00 | Loss_d: 7.8471e-01 | Loss_e: 7.9509e-01\n",
            "Step: NaN | Loss: 1.5169e-01 | Loss_d: 1.1320e-02 | Loss_e: 1.5421e-01\n",
            "Step: NaN | Loss: 1.0236e-01 | Loss_d: 5.9442e-03 | Loss_e: 1.2101e-01\n",
            "Step: NaN | Loss: 1.0234e-01 | Loss_d: 5.9168e-03 | Loss_e: 1.2125e-01\n",
            "Step: NaN | Loss: 1.0237e-01 | Loss_d: 5.8259e-03 | Loss_e: 1.2220e-01\n",
            "Step: NaN | Loss: 1.0233e-01 | Loss_d: 5.8809e-03 | Loss_e: 1.2158e-01\n",
            "Step: NaN | Loss: 1.0229e-01 | Loss_d: 5.8896e-03 | Loss_e: 1.2143e-01\n",
            "Step: NaN | Loss: 1.0217e-01 | Loss_d: 5.9278e-03 | Loss_e: 1.2083e-01\n",
            "Step: NaN | Loss: 1.0213e-01 | Loss_d: 6.2047e-03 | Loss_e: 1.1799e-01\n",
            "Step: NaN | Loss: 1.0202e-01 | Loss_d: 6.0594e-03 | Loss_e: 1.1926e-01\n",
            "Step: NaN | Loss: 1.0159e-01 | Loss_d: 6.0457e-03 | Loss_e: 1.1863e-01\n",
            "Step: NaN | Loss: 1.0057e-01 | Loss_d: 6.1094e-03 | Loss_e: 1.1620e-01\n",
            "Step: NaN | Loss: 1.1725e-01 | Loss_d: 1.0019e-02 | Loss_e: 1.0651e-01\n",
            "Step: NaN | Loss: 1.0051e-01 | Loss_d: 6.1564e-03 | Loss_e: 1.1562e-01\n",
            "Step: NaN | Loss: 9.9243e-02 | Loss_d: 5.7085e-03 | Loss_e: 1.1787e-01\n",
            "Step: NaN | Loss: 9.7689e-02 | Loss_d: 4.3520e-03 | Loss_e: 1.2869e-01\n",
            "Step: NaN | Loss: 9.7519e-02 | Loss_d: 4.6367e-03 | Loss_e: 1.2555e-01\n",
            "Step: NaN | Loss: 9.5513e-02 | Loss_d: 4.8186e-03 | Loss_e: 1.2019e-01\n",
            "Step: NaN | Loss: 1.0084e-01 | Loss_d: 6.7821e-03 | Loss_e: 1.0995e-01\n",
            "Step: NaN | Loss: 9.4677e-02 | Loss_d: 5.0994e-03 | Loss_e: 1.1591e-01\n",
            "Step: NaN | Loss: 9.3706e-02 | Loss_d: 5.1181e-03 | Loss_e: 1.1401e-01\n",
            "Step: NaN | Loss: 9.3150e-02 | Loss_d: 5.5995e-03 | Loss_e: 1.0822e-01\n",
            "Step: NaN | Loss: 9.2674e-02 | Loss_d: 5.3025e-03 | Loss_e: 1.1035e-01\n",
            "Step: NaN | Loss: 9.0838e-02 | Loss_d: 4.6827e-03 | Loss_e: 1.1331e-01\n",
            "Step: NaN | Loss: 1.1748e-01 | Loss_d: 6.5112e-03 | Loss_e: 1.4200e-01\n",
            "Step: NaN | Loss: 9.0836e-02 | Loss_d: 4.6691e-03 | Loss_e: 1.1344e-01\n",
            "Step: NaN | Loss: 8.9921e-02 | Loss_d: 3.9113e-03 | Loss_e: 1.1941e-01\n",
            "Step: NaN | Loss: 8.9767e-02 | Loss_d: 4.0926e-03 | Loss_e: 1.1732e-01\n",
            "Step: NaN | Loss: 8.9439e-02 | Loss_d: 3.9469e-03 | Loss_e: 1.1820e-01\n",
            "Step: NaN | Loss: 9.0507e-02 | Loss_d: 3.6479e-03 | Loss_e: 1.2307e-01\n",
            "Step: NaN | Loss: 8.9345e-02 | Loss_d: 3.8441e-03 | Loss_e: 1.1906e-01\n",
            "Step: NaN | Loss: 8.8547e-02 | Loss_d: 3.8614e-03 | Loss_e: 1.1748e-01\n",
            "Step: NaN | Loss: 8.6797e-02 | Loss_d: 4.1092e-03 | Loss_e: 1.1192e-01\n",
            "Step: NaN | Loss: 1.3039e-01 | Loss_d: 1.1441e-02 | Loss_e: 1.1546e-01\n",
            "Step: NaN | Loss: 8.6756e-02 | Loss_d: 4.1581e-03 | Loss_e: 1.1136e-01\n",
            "Step: NaN | Loss: 8.5628e-02 | Loss_d: 4.2540e-03 | Loss_e: 1.0841e-01\n",
            "Step: NaN | Loss: 8.6867e-02 | Loss_d: 4.9937e-03 | Loss_e: 1.0320e-01\n",
            "Step: NaN | Loss: 8.5070e-02 | Loss_d: 4.4644e-03 | Loss_e: 1.0532e-01\n",
            "Step: NaN | Loss: 8.5088e-02 | Loss_d: 4.2141e-03 | Loss_e: 1.0786e-01\n",
            "Step: NaN | Loss: 8.4430e-02 | Loss_d: 4.2747e-03 | Loss_e: 1.0609e-01\n",
            "Step: NaN | Loss: 8.3565e-02 | Loss_d: 4.5111e-03 | Loss_e: 1.0220e-01\n",
            "Step: NaN | Loss: 8.9290e-02 | Loss_d: 6.6277e-03 | Loss_e: 9.1129e-02\n",
            "Step: NaN | Loss: 8.3464e-02 | Loss_d: 4.6576e-03 | Loss_e: 1.0056e-01\n",
            "Step: NaN | Loss: 8.2955e-02 | Loss_d: 4.6161e-03 | Loss_e: 1.0008e-01\n",
            "Step: NaN | Loss: 8.2950e-02 | Loss_d: 4.6142e-03 | Loss_e: 1.0009e-01\n",
            "Step: NaN | Loss: 8.2601e-02 | Loss_d: 4.4864e-03 | Loss_e: 1.0075e-01\n",
            "Step: NaN | Loss: 8.3176e-02 | Loss_d: 4.2369e-03 | Loss_e: 1.0426e-01\n",
            "Step: NaN | Loss: 8.2439e-02 | Loss_d: 4.3655e-03 | Loss_e: 1.0167e-01\n",
            "Step: NaN | Loss: 8.0768e-02 | Loss_d: 4.6565e-03 | Loss_e: 9.5818e-02\n",
            "Step: NaN | Loss: 8.8827e-02 | Loss_d: 7.4823e-03 | Loss_e: 8.1768e-02\n",
            "Step: NaN | Loss: 8.0247e-02 | Loss_d: 4.8994e-03 | Loss_e: 9.2471e-02\n",
            "Step: NaN | Loss: 7.7186e-02 | Loss_d: 4.3824e-03 | Loss_e: 9.2245e-02\n",
            "Step: NaN | Loss: 1.4475e-01 | Loss_d: 1.2319e-02 | Loss_e: 1.3198e-01\n",
            "Step: NaN | Loss: 7.7184e-02 | Loss_d: 4.3801e-03 | Loss_e: 9.2265e-02\n",
            "Step: NaN | Loss: 7.6937e-02 | Loss_d: 4.0367e-03 | Loss_e: 9.5264e-02\n",
            "Step: NaN | Loss: 7.6485e-02 | Loss_d: 4.1110e-03 | Loss_e: 9.3723e-02\n",
            "Step: NaN | Loss: 7.6070e-02 | Loss_d: 4.0712e-03 | Loss_e: 9.3388e-02\n",
            "Step: NaN | Loss: 7.7776e-02 | Loss_d: 4.3605e-03 | Loss_e: 9.3504e-02\n",
            "Step: NaN | Loss: 7.5963e-02 | Loss_d: 4.0683e-03 | Loss_e: 9.3230e-02\n",
            "Step: NaN | Loss: 7.5688e-02 | Loss_d: 4.1291e-03 | Loss_e: 9.2137e-02\n",
            "Step: NaN | Loss: 7.5747e-02 | Loss_d: 4.5363e-03 | Loss_e: 8.8170e-02\n",
            "Step: NaN | Loss: 7.5482e-02 | Loss_d: 4.2858e-03 | Loss_e: 9.0207e-02\n",
            "Step: NaN | Loss: 7.5064e-02 | Loss_d: 4.2624e-03 | Loss_e: 8.9704e-02\n",
            "Step: NaN | Loss: 7.4180e-02 | Loss_d: 4.2583e-03 | Loss_e: 8.8187e-02\n",
            "Step: NaN | Loss: 8.7895e-02 | Loss_d: 6.2530e-03 | Loss_e: 9.2418e-02\n",
            "Step: NaN | Loss: 7.4154e-02 | Loss_d: 4.2762e-03 | Loss_e: 8.7961e-02\n",
            "Step: NaN | Loss: 7.2225e-02 | Loss_d: 4.1232e-03 | Loss_e: 8.6091e-02\n",
            "Step: NaN | Loss: 8.4019e-02 | Loss_d: 6.3547e-03 | Loss_e: 8.4567e-02\n",
            "Step: NaN | Loss: 7.1663e-02 | Loss_d: 4.1044e-03 | Loss_e: 8.5288e-02\n",
            "Step: NaN | Loss: 7.0566e-02 | Loss_d: 3.7059e-03 | Loss_e: 8.7340e-02\n",
            "Step: NaN | Loss: 7.0535e-02 | Loss_d: 3.7470e-03 | Loss_e: 8.6874e-02\n",
            "Step: NaN | Loss: 6.9852e-02 | Loss_d: 3.6617e-03 | Loss_e: 8.6523e-02\n",
            "Step: NaN | Loss: 6.9708e-02 | Loss_d: 3.6489e-03 | Loss_e: 8.6397e-02\n",
            "Step: NaN | Loss: 6.8273e-02 | Loss_d: 3.9743e-03 | Loss_e: 8.0614e-02\n",
            "Step: NaN | Loss: 8.9987e-02 | Loss_d: 8.1657e-03 | Loss_e: 7.6979e-02\n",
            "Step: NaN | Loss: 6.8238e-02 | Loss_d: 4.0315e-03 | Loss_e: 7.9980e-02\n",
            "Step: NaN | Loss: 6.6498e-02 | Loss_d: 3.9896e-03 | Loss_e: 7.7332e-02\n",
            "Step: NaN | Loss: 6.5968e-02 | Loss_d: 3.8557e-03 | Loss_e: 7.7737e-02\n",
            "Step: NaN | Loss: 6.4857e-02 | Loss_d: 3.8405e-03 | Loss_e: 7.5930e-02\n",
            "Step: NaN | Loss: 6.9329e-02 | Loss_d: 5.3666e-03 | Loss_e: 6.8552e-02\n",
            "Step: NaN | Loss: 6.4512e-02 | Loss_d: 3.9141e-03 | Loss_e: 7.4586e-02\n",
            "Step: NaN | Loss: 6.2551e-02 | Loss_d: 3.6217e-03 | Loss_e: 7.4052e-02\n",
            "Step: NaN | Loss: 5.8486e-02 | Loss_d: 3.0061e-03 | Loss_e: 7.3042e-02\n",
            "Step: NaN | Loss: 2.3134e-01 | Loss_d: 2.7911e-02 | Loss_e: 1.2872e-01\n",
            "Step: NaN | Loss: 5.8459e-02 | Loss_d: 3.0020e-03 | Loss_e: 7.3036e-02\n",
            "Step: NaN | Loss: 5.2175e-02 | Loss_d: 2.8826e-03 | Loss_e: 6.3152e-02\n",
            "Step: NaN | Loss: 6.9711e-02 | Loss_d: 5.6107e-03 | Loss_e: 6.6784e-02\n",
            "Step: NaN | Loss: 4.9132e-02 | Loss_d: 3.0219e-03 | Loss_e: 5.6395e-02\n",
            "Step: NaN | Loss: 4.6275e-02 | Loss_d: 3.2665e-03 | Loss_e: 4.8912e-02\n",
            "Step: NaN | Loss: 4.5602e-02 | Loss_d: 3.0277e-03 | Loss_e: 5.0113e-02\n",
            "Step: NaN | Loss: 4.8321e-02 | Loss_d: 3.8905e-03 | Loss_e: 4.6279e-02\n",
            "Step: NaN | Loss: 4.5074e-02 | Loss_d: 3.1771e-03 | Loss_e: 4.7689e-02\n",
            "Step: NaN | Loss: 4.4449e-02 | Loss_d: 3.1049e-03 | Loss_e: 4.7308e-02\n",
            "Step: NaN | Loss: 4.4433e-02 | Loss_d: 3.1024e-03 | Loss_e: 4.7305e-02\n",
            " Total iterations: 97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotter\n",
        "\n",
        "######################################################################\n",
        "############################# Plotting ###############################\n",
        "######################################################################\n",
        "\n",
        "class PINN_Plotter:\n",
        "    def __init__(self, X_star, pred_u, loss_dict = None, rootdir = Path(), filename = \"loss.png\", methods = [\"adam\"], n_iters = [1],\n",
        "                 dpi = 300, precision = jnp.float64, fontsize = {'title': 15, 'subtitle': 12}):\n",
        "        self.X_star = X_star #input coordinates\n",
        "        self.pred_u = pred_u #prediction function based on neural net\n",
        "        self.loss_dict = loss_dict #dictionary containing loss over time, with 'loss', 'loss_e', and 'loss_d'\n",
        "        self.rootdir = rootdir\n",
        "        self.filename = str(rootdir.joinpath(filename))\n",
        "        self.methods = methods\n",
        "        self.n_iters = n_iters\n",
        "        self.dpi = dpi\n",
        "        self.precision = precision\n",
        "        self.fontsize = fontsize\n",
        "\n",
        "    def plot_loss(self):#, filename = \"/content/drive/MyDrive/loss.png\", method = \"\", n_iter = \"\", include_equation_loss = True):\n",
        "        figsize_loss_e = [8, 4]\n",
        "        figsize = [3, 3]\n",
        "\n",
        "        ## LOSS\n",
        "        loss = self.loss_dict['loss']\n",
        "        loss_d = self.loss_dict['loss_d']\n",
        "        if \"loss_e\" in self.loss_dict:\n",
        "            loss_e = self.loss_dict['loss_e']\n",
        "            fig = plt.figure(figsize = figsize_loss_e, dpi = self.dpi)\n",
        "            ax = plt.subplot(131)\n",
        "        else:\n",
        "            fig = plt.figure(figsize = figsize, dpi = self.dpi)\n",
        "            ax = plt.subplot(111)\n",
        "\n",
        "        # Set title of figure\n",
        "        fig.suptitle(f'Loss', fontsize = self.fontsize['title'])#, method = {self.method}, n_iter = {self.n_iter}', fontsize = 15)\n",
        "\n",
        "        def ax_properties(ax):\n",
        "            ax.axhline(y=0, color = \"k\", linewidth = \"0.5\") #Add x- and y-axis line\n",
        "            ax.axvline(x=0, color = \"k\", linewidth = \"0.5\")\n",
        "            ax.set_yscale('log')\n",
        "            ax.set_xlabel('Iteration', fontsize = 12)\n",
        "            ax.set_xlim(left = 0, right = jnp.sum(jnp.array(self.n_iters)))\n",
        "            return ax\n",
        "\n",
        "        def plot_loss(ax, loss, y_label):\n",
        "            ax.set_ylabel(y_label, fontsize = self.fontsize['subtitle'], rotation = 90)\n",
        "            n = 0\n",
        "            # Plot loss, labeled by optimization method\n",
        "            for i, n_iter in enumerate(self.n_iters):\n",
        "                ax.plot(jnp.arange(n, n + n_iter), loss[n:n + n_iter], '-', linewidth = 2, label = self.methods[i])\n",
        "                n += n_iter\n",
        "\n",
        "        plot_loss(ax = ax, loss = loss, y_label = \"Total loss\")\n",
        "        ax.legend()\n",
        "\n",
        "        if \"loss_e\" in self.loss_dict:\n",
        "            ax = plt.subplot(132)\n",
        "            ax = ax_properties(ax)\n",
        "            plot_loss(ax = ax, loss = loss_d, y_label = \"Data loss\")\n",
        "\n",
        "            ax = plt.subplot(133)\n",
        "            ax = ax_properties(ax)\n",
        "            plot_loss(ax = ax, loss = loss_e, y_label = \"Equation loss\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.filename)\n",
        "        plt.show()\n",
        "        plt.close('all')\n",
        "\n",
        "    def plot_1d_ode(self): #description, model, target_function, data_type, N_p):\n",
        "        # plot_loss(loss = model.loss, loss_d = model.loss_d, loss_e = model.loss_e, method = method, n_iter = n_iter, include_equation_loss = False, filename = f\"/content/drive/MyDrive/7_11/{description}_loss.png\")\n",
        "        fig = plt.figure(figsize = [10, 10], dpi = self.dpi)\n",
        "\n",
        "        # The analytic solution\n",
        "        ax = plt.subplot(321)\n",
        "        # Add x- and y-axis line\n",
        "        ax.axhline(y=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.axvline(x=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.margins(x = 0)\n",
        "        ax.plot(model.x_u, model.u, 'x', label = 'training data')\n",
        "\n",
        "        x_star = np.linspace(model.lb, model.ub, N_p)[:,None]\n",
        "        x_star = tf.cast(x_star, dtype = data_type)\n",
        "        u_star = target_function(x_star)\n",
        "        u_pred, f_pred = model.predict(x_star)\n",
        "        ax.plot(x_star, u_star, 'b-', linewidth = 2, label = \"exact\")\n",
        "        ax.plot(x_star, u_pred, 'r--', linewidth = 2, label = 'predict')\n",
        "        ax.set_xlabel('$x$', fontsize = 12)\n",
        "        ax.set_ylabel('$u$', fontsize = 12, rotation = 0)\n",
        "        ax.set_title(f'solution, gamma = {gamma}, {method}', fontsize = 12)\n",
        "        ax.legend()\n",
        "\n",
        "        # FFT of NN prediction\n",
        "        ax = plt.subplot(322)\n",
        "        # Add x- and y-axis line\n",
        "        ax.axhline(y=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.axvline(x=0, color = \"k\", linewidth = \"0.5\")\n",
        "\n",
        "        ax.plot(np.linspace(1, len(model.loss), len(model.loss)), model.loss, 'b-', linewidth = 2)\n",
        "        ax.margins(x = 0)\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_xlabel('iteration', fontsize = 12)\n",
        "        ax.set_ylabel('total loss', fontsize = 12, rotation = 90)\n",
        "        ax.set_title(f'loss, method = {method}, n_iter = {n_iter}', fontsize = 15)\n",
        "\n",
        "        # N = 256\n",
        "        # t = np.linspace(0, 2, N)\n",
        "        # timestep = N / (2 - 0)\n",
        "        # frequency = 5\n",
        "        # f = np.sin(frequency * 2 * np.pi * t) # 1 cycle/second: https://numpy.org/doc/stable/reference/generated/numpy.fft.fftfreq.html\n",
        "        # ft = np.fft.fft(f)\n",
        "        # freq = np.fft.fftfreq(n = N, d = 1 / timestep)\n",
        "        # ax.set_xlim(0, 100)\n",
        "        # ax.stem(freq, ft.real**2 + ft.imag**2, basefmt = 'k')\n",
        "\n",
        "        # The error\n",
        "        ax = plt.subplot(323)\n",
        "        ax.axhline(y=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.axvline(x=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.margins(x = 0)\n",
        "        u_train_pred = model.predict(model.x_u)[0]\n",
        "        ax.plot(model.x_u, model.u - u_train_pred, 'b-', linewidth = 2, label = '$u_g(x) - u_0(x)$') # TODO : check whether to plot using the function or using the data points provided\n",
        "        ax.set_title(f'first-stage residue')\n",
        "\n",
        "        # Frequencies of error\n",
        "        ax = plt.subplot(324)\n",
        "        N = model.x_u.shape[0]\n",
        "        timestep = N / (model.ub - model.lb)\n",
        "        print(f\"timestep: {timestep}\")\n",
        "        ft = np.fft.fft(model.u - u_train_pred)\n",
        "        freq = np.fft.fftfreq(n = N, d = 1 / timestep)\n",
        "        ax.stem(freq, ft.real**2 + ft.imag**2, basefmt = 'k', markerfmt = 'o')\n",
        "        # ifft = np.fft.ifft(ft)\n",
        "        ax.set_xlim(0, 20)\n",
        "        # ax.plot(model.x_u.numpy().flatten(), ifft, 'b-', label = \"IFFT\")\n",
        "        # ax.plot(model.x_u, model.u - u_train_pred, 'r--', label = \"original signal\")\n",
        "        # ax.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "    def plot_1d_pinn(description, model, target_function, data_type, title, N_p, method = \"\", n_iter = \"\", gamma = \"\"):\n",
        "        plot_loss(model = model, method = method, n_iter = n_iter, include_equation_loss = True, filename = f\"/content/drive/MyDrive/7_19/{description}_loss.png\")\n",
        "        fig = plt.figure(figsize = [9, 6], dpi = 300)\n",
        "\n",
        "        # The analytic solution\n",
        "        ax = plt.subplot(321)\n",
        "        # Add x- and y-axis line\n",
        "        ax.axhline(y=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.axvline(x=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.margins(x = 0)\n",
        "        ax.plot(model.x_u, model.u, 'x', label = 'training data')\n",
        "\n",
        "        x_star = np.linspace(model.lb, model.ub, N_p)[:,None]\n",
        "        x_star = tf.cast(x_star, dtype = data_type)\n",
        "        u_star = target_function(x_star)\n",
        "        u_pred, f_pred = model.predict(x_star)\n",
        "        ax.plot(x_star, u_star, 'b-', linewidth = 2, label = \"exact\")\n",
        "        ax.plot(x_star, u_pred, 'r--', linewidth = 2, label = 'predict')\n",
        "        ax.set_xlabel('$x$', fontsize = 12)\n",
        "        ax.set_ylabel('$u$', fontsize = 12, rotation = 0)\n",
        "        ax.set_title(f'solution, gamma = {gamma}, {method}', fontsize = 12)\n",
        "        ax.legend()\n",
        "\n",
        "        # FFT of NN prediction\n",
        "        ax = plt.subplot(322)\n",
        "        # Add x- and y-axis line\n",
        "        ax.axhline(y=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.axvline(x=0, color = \"k\", linewidth = \"0.5\")\n",
        "\n",
        "        ax.plot(np.linspace(1, len(model.loss), len(model.loss)), model.loss, 'b-', linewidth = 2)\n",
        "        ax.margins(x = 0)\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_xlabel('iteration', fontsize = 12)\n",
        "        ax.set_ylabel('total loss', fontsize = 12, rotation = 90)\n",
        "        ax.set_title(f'loss, method = {method}, n_iter = {n_iter}', fontsize = 15)\n",
        "\n",
        "        # The error\n",
        "        ax = plt.subplot(323)\n",
        "        ax.axhline(y=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.axvline(x=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.margins(x = 0)\n",
        "        ax.plot(x_star, f_pred, 'b-', linewidth = 2, label = '$u_g(x) - u_0(x)$') # TODO : check whether to plot using the function or using the data points provided\n",
        "        ax.set_title(f'equation residue')\n",
        "\n",
        "        # Frequencies of error\n",
        "        ax = plt.subplot(324)\n",
        "        timestep = N_p / (model.ub - model.lb)\n",
        "        ft = np.fft.fft(f_pred.numpy().flatten())\n",
        "        freq = np.fft.fftfreq(n = N_p, d = 1 / timestep)\n",
        "        ax.stem(freq, ft.real**2 + ft.imag**2, basefmt = 'k', markerfmt = 'o')\n",
        "        # ifft = np.fft.ifft(ft)\n",
        "        ax.set_xlim(0, 20)\n",
        "        # ax.plot(model.x_u.numpy().flatten(), ifft, 'b-', label = \"IFFT\")\n",
        "        # ax.plot(model.x_u, model.u - u_train_pred, 'r--', label = \"original signal\")\n",
        "        # ax.legend()\n",
        "\n",
        "        # Analytic solution error\n",
        "        ax = plt.subplot(325)\n",
        "        ax.axhline(y=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.axvline(x=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.margins(x = 0)\n",
        "        # u_train_pred = model.predict(model.x_u)[0]\n",
        "        error = (u_star - u_pred).numpy().flatten()\n",
        "        ax.plot(x_star, error, 'b-', linewidth = 2, label = '$u_g(x) - u_0(x)$') # TODO : check whether to plot using the function or using the data points provided\n",
        "        ax.set_title(f'$e_1(x)$')\n",
        "\n",
        "        # Frequencies of error\n",
        "        ax = plt.subplot(326)\n",
        "        timestep = N_p / (model.ub - model.lb)\n",
        "        ft = np.fft.fft(error)\n",
        "        freq = np.fft.fftfreq(n = N_p, d = 1 / timestep)\n",
        "        ax.stem(freq, np.abs(ft), basefmt = 'k', markerfmt = 'o')\n",
        "        # ifft = np.fft.ifft(ft)\n",
        "        ax.set_xlim(0, 20)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    \"\"\"\n",
        "    Helper function to plot a snapshot of turbulence\n",
        "    \"\"\"\n",
        "    def plot_results(filename, title, fontsize, u_star, u_pred, error, kappa, X, Y, loss, method, n_iter, timestep, u_min, u_max, err_min, err_max, freq_x, freq_y, f_max, dpi = 200):\n",
        "        fig = plt.figure(figsize = [4, 8], dpi = dpi)\n",
        "        fig.suptitle(title, fontsize = fontsize)\n",
        "\n",
        "        # Contour map of actual solution\n",
        "        ax = fig.add_subplot(5, 2, 1)\n",
        "        ax.set_xlabel('x', fontsize = fontsize)\n",
        "        ax.set_ylabel('y', fontsize = fontsize)\n",
        "        ax.set_title('target function', fontsize = fontsize)\n",
        "        ax.set_aspect('equal')\n",
        "\n",
        "        h = ax.contourf(X, Y, u_star, levels = 100, cmap = 'jet', vmin = u_min, vmax = u_max)\n",
        "        # 3D plot\n",
        "        ax = fig.add_subplot(5, 2, 2, projection='3d')\n",
        "        ax.view_init(elev=30, azim=45 + 180)\n",
        "        surf = ax.plot_surface(X, Y, u_star, rstride=1, cstride=1, linewidth=0, antialiased=False, cmap = 'jet', vmin = u_min, vmax = u_max)\n",
        "        ax.set_xlabel('$x$', fontsize = fontsize)\n",
        "        ax.set_ylabel('$y$', fontsize = fontsize)\n",
        "        plt.colorbar(surf, ax = ax, pad = 0.2)\n",
        "\n",
        "        # Plot model prediction\n",
        "        ax = fig.add_subplot(5, 2, 3)\n",
        "        ax.set_xlabel('x', fontsize = fontsize)\n",
        "        ax.set_ylabel('y', fontsize = fontsize)\n",
        "        ax.set_title(f'model prediction, kappa = {kappa}', fontsize = fontsize)\n",
        "        h = ax.contourf(X, Y, u_pred, levels = 100, cmap = 'jet', vmin = u_min, vmax = u_max)\n",
        "        ax.set_aspect('equal')\n",
        "\n",
        "        # 3D plot\n",
        "        ax = fig.add_subplot(5, 2, 4, projection='3d')\n",
        "        ax.view_init(elev=30, azim=45 + 180)\n",
        "        surf = ax.plot_surface(X, Y, u_pred, rstride=1, cstride=1, linewidth=0, antialiased=False, cmap = 'jet', vmin = u_min, vmax = u_max)\n",
        "        ax.set_xlabel('$x$', fontsize = fontsize)\n",
        "        ax.set_ylabel('$y$', fontsize = fontsize)\n",
        "        plt.colorbar(surf, ax = ax, pad = 0.2)\n",
        "\n",
        "        # Plot error\n",
        "        ax = fig.add_subplot(5, 2, 5)\n",
        "        ax.set_xlabel('x', fontsize = fontsize)\n",
        "        ax.set_ylabel('y', fontsize = fontsize)\n",
        "        ax.set_title('error', fontsize = fontsize)\n",
        "        h = ax.contourf(X, Y, error, levels = 100, cmap = 'jet', vmin = err_min, vmax = err_max)\n",
        "        ax.set_aspect('equal')\n",
        "\n",
        "        # 3D plot\n",
        "        ax = fig.add_subplot(5, 2, 6, projection='3d')\n",
        "        ax.view_init(elev=30, azim=45 + 180)\n",
        "        surf = ax.plot_surface(X, Y, error, rstride=1, cstride=1, linewidth=0, antialiased=False, cmap = 'jet', vmin = err_min, vmax = err_max)\n",
        "        ax.set_xlabel('$x$', fontsize = fontsize)\n",
        "        ax.set_ylabel('$y$', fontsize = fontsize)\n",
        "        plt.colorbar(surf, ax = ax, pad = 0.2)\n",
        "\n",
        "        # Loss\n",
        "        ax = fig.add_subplot(5, 2, 7)\n",
        "        ax.axhline(y=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.axvline(x=0, color = \"k\", linewidth = \"0.5\")\n",
        "        ax.plot(np.linspace(1, len(loss), len(loss)), loss, 'b-', linewidth = 2)\n",
        "        ax.margins(x = 0)\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_xlabel('iteration', fontsize = fontsize)\n",
        "        ax.set_ylabel('total loss', rotation = 90, fontsize = fontsize)\n",
        "        ax.set_title(f'loss, method = {method}, n_iter = {n_iter}', fontsize = fontsize)\n",
        "\n",
        "        # Frequencies\n",
        "        ax = fig.add_subplot(5, 2, 8)\n",
        "        ax.set_xlim(xmin = 0, xmax = f_max)\n",
        "        ax.set_ylim(ymin = 0, ymax = f_max)\n",
        "        FS = np.fft.fftn(error)\n",
        "        ax.pcolormesh(freq_x, freq_y, np.abs(np.fft.fftshift(FS)), shading = \"auto\")\n",
        "        plt.tight_layout()\n",
        "        FS_abs = np.abs(np.fft.fftshift(FS))\n",
        "        argmax = np.unravel_index(FS_abs.argmax(), FS_abs.shape)\n",
        "        f_d = np.array([freq_x[argmax[1]], freq_y[argmax[0]]])\n",
        "\n",
        "        plt.savefig(f\"{filename}.png\", pad_inches = 0.1)\n",
        "        plt.show()\n",
        "        print(f\"Figure saved to {filename}.png\")\n",
        "        plt.close()\n",
        "        return f_d\n",
        "\n",
        "    def plot_2d_pde(filename, model, target_function, data_type, title, N_ux, N_uy, vmin, vmax, method, n_iter, f_max, fontsize):\n",
        "        lb = model.lb\n",
        "        ub = model.ub\n",
        "        x = np.linspace(lb[0], ub[0], N_ux)\n",
        "        y = np.linspace(lb[1], ub[1], N_uy)\n",
        "        X, Y = np.meshgrid(x,y)\n",
        "        x_star = np.hstack((X.flatten()[:,None], Y.flatten()[:,None]))\n",
        "        x_star = tf.cast(x_star, dtype = data_type)\n",
        "        u_star = target_function(x_star)\n",
        "        u_star = tf.cast(u_star, dtype = data_type)\n",
        "        u_star = u_star.numpy().reshape(N_uy, N_ux)\n",
        "        u_pred = model.predict(x_star)[0] * model.eps\n",
        "        u_pred = u_pred.numpy().reshape(N_uy, N_ux)\n",
        "\n",
        "        error = u_star - u_pred\n",
        "        u_min = np.min([u_star, u_pred])\n",
        "        u_max = np.max([u_star, u_pred])\n",
        "        err_min = np.min(error)\n",
        "        err_max = np.max(error)\n",
        "        timestep = np.array([N_ux, N_uy]) / (model.ub - model.lb)\n",
        "        freq_x = np.fft.fftshift(np.fft.fftfreq(n = N_ux, d = 1 / timestep[0]))\n",
        "        freq_y = np.fft.fftshift(np.fft.fftfreq(n = N_uy, d = 1 / timestep[1]))\n",
        "        FS = np.fft.fftn(error)\n",
        "        plt, f_d = plot_results(filename = filename, title = title, fontsize = fontsize, u_star = u_star, u_pred = u_pred, error = error,\n",
        "                            kappa = model.kappa, X = X, Y = Y, loss = model.loss, method = method, n_iter = n_iter, timestep = timestep,\n",
        "                            u_min = u_min, u_max = u_max, err_min = err_min, err_max = err_max,\n",
        "                            freq_x = freq_x, freq_y = freq_y, f_max = f_max)\n",
        "\n",
        "        return f_d\n",
        "\n",
        "    def plot_3d_pde(filename, model, target_function, data_type, title, N_ux, N_uy, N_ut, vmin, vmax, method, n_iter, f_max, fontsize):\n",
        "        # plot_loss(loss = model.loss, loss_d = model.loss_d, loss_e = model.loss_e, method = method, n_iter = n_iter, include_equation_loss = False, filename = f\"/content/drive/MyDrive/7_11/{description}_loss.png\")\n",
        "\n",
        "        # The analytic solution\n",
        "        lb = model.lb\n",
        "        ub = model.ub\n",
        "        x = np.linspace(lb[0], ub[0], N_ux) # n_x samples from [0, 2pi), shape: (n_x, 1)\n",
        "        y = np.linspace(lb[1], ub[1], N_uy) # n_t samples from [0, 10), shape: (n_t, 1)\n",
        "        t = np.linspace(lb[2], ub[2], N_ut)\n",
        "        X, Y, T = np.meshgrid(x, y, t)\n",
        "        x_star = np.array([[x0, y0, t0] for x0 in x for y0 in y for t0 in t]) # create array of (x, y) values\\\n",
        "        x_star = tf.cast(x_star, dtype = data_type)\n",
        "        u_star = target_function(x_star)\n",
        "        u_star = tf.cast(u_star, dtype = data_type)\n",
        "        u_star = u_star.numpy().reshape(N_uy, N_ux, N_ut)\n",
        "        u_pred = model.predict(x_star)[0] * model.eps\n",
        "        u_pred = tf.cast(u_pred, dtype = data_type)\n",
        "        u_pred = u_pred.numpy().reshape(N_uy, N_ux, N_ut)\n",
        "        error = u_star - u_pred\n",
        "        u_min = np.min([u_star, u_pred])\n",
        "        u_max = np.max([u_star, u_pred])\n",
        "        err_min = np.min(error)\n",
        "        err_max = np.max(error)\n",
        "\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        timestep = np.array([N_ux, N_uy]) / (model.ub[0:2] - model.lb[0:2])\n",
        "        # FS = np.fft.fftn(error[:, :, i])\n",
        "        freq_x = np.fft.fftshift(np.fft.fftfreq(n = N_ux, d = 1 / timestep[0]))\n",
        "        freq_y = np.fft.fftshift(np.fft.fftfreq(n = N_uy, d = 1 / timestep[1]))\n",
        "\n",
        "        f_d_list = np.array([])\n",
        "        for i in range(N_ut):\n",
        "            f_d = plot_results(filename = f\"{filename}_{i:04n}\", title = title, fontsize = fontsize, u_star = u_star[:, :, i], u_pred = u_pred[:, :, i],\n",
        "                            error = error[:, :, i], kappa = model.kappa, X = X, Y = Y, loss = model.loss, method = method, n_iter = n_iter, timestep = timestep,\n",
        "                            u_min = u_min, u_max = u_max, err_min = err_min, err_max = err_max,\n",
        "                            freq_x = freq_x, freq_y = freq_y, f_max = f_max)\n",
        "\n",
        "            f_d_list = np.append(f_d_list, f_d)\n",
        "        print(f\"f_d_list returning: {f_d_list}\")\n",
        "        return f_d_list\n",
        "\n",
        "    def plot_pde(dim, filename, model, target_function, data_type, title, N_ux, N_uy, vmin = -0.4, vmax = 1, method = \"\", n_iter = \"\", f_max = 5, fontsize = 6, N_ut = 0):\n",
        "        if dim == 2:\n",
        "            return plot_2d_pde(filename, model, target_function, data_type, title, N_ux, N_uy, vmin, vmax, method, n_iter, f_max, fontsize)\n",
        "        elif dim == 3:\n",
        "            return plot_3d_pde(filename, model, target_function, data_type, title, N_ux, N_uy, N_ut, vmin, vmax, method, n_iter, f_max, fontsize)\n",
        "\n",
        "    def plot_model_sum(self, filename, title, models, target_function, N_ux, N_uy, N_ut, data_type = jnp.float64, dim = 2, fontsize = 10):\n",
        "        dpi = 100\n",
        "        model = models[0]\n",
        "        lb = model.lb\n",
        "        ub = model.ub\n",
        "        if dim == 2:\n",
        "            x = np.linspace(lb[0], ub[0], N_ux)\n",
        "            y = np.linspace(lb[1], ub[1], N_uy)\n",
        "            X, Y = np.meshgrid(x,y)\n",
        "            x_star = np.hstack((X.flatten()[:,None], Y.flatten()[:,None]))\n",
        "            x_star = tf.cast(x_star, dtype = data_type)\n",
        "            u_star = target_function(x_star)\n",
        "            u_star = tf.cast(u_star, dtype = data_type).numpy().reshape(N_uy, N_ux)\n",
        "            model_sum = 0\n",
        "            for m in models:\n",
        "                model_sum += m.eps * m.predict(x_star)[0].numpy().reshape(N_uy, N_ux)\n",
        "\n",
        "        if dim == 3:\n",
        "            x = np.linspace(lb[0], ub[0], N_ux) # n_x samples from [0, 2pi), shape: (n_x, 1)\n",
        "            y = np.linspace(lb[1], ub[1], N_uy) # n_t samples from [0, 10), shape: (n_t, 1)\n",
        "            t = np.linspace(lb[2], ub[2], N_ut)\n",
        "            X, Y, T = np.meshgrid(x, y, t)\n",
        "            x_star = np.array([[x0, y0, t0] for x0 in x for y0 in y for t0 in t]) # create array of (x, y) values\\\n",
        "            x_star = tf.cast(x_star, dtype = data_type)\n",
        "            model_sum = 0\n",
        "            for m in models:\n",
        "                model_sum += m.eps * m.predict(x_star)[0].numpy().reshape(N_uy, N_ux, N_ut)\n",
        "            u_star = target_function(x_star)\n",
        "            u_star = tf.cast(u_star, dtype = data_type).numpy().reshape(N_uy, N_ux, N_ut)\n",
        "            X, Y = np.meshgrid(x, y)\n",
        "\n",
        "        error = model_sum - u_star\n",
        "        err_min = np.min(error)\n",
        "        err_max = np.max(error)\n",
        "        u_min = np.min([model_sum, u_star])\n",
        "        u_max = np.max([model_sum, u_star])\n",
        "\n",
        "        for i in range(N_ut):\n",
        "            if dim == 2:\n",
        "                ms = model_sum\n",
        "            us = u_star\n",
        "            if dim == 3:\n",
        "                ms = model_sum[:, :, i]\n",
        "            us = u_star[:, :, i]\n",
        "            fig = plt.figure(figsize = [9, 3], dpi = dpi)\n",
        "\n",
        "            rmse = tf.sqrt(tf.reduce_mean(tf.square(us - ms)))\n",
        "            fig.suptitle(f\"RMSE: {rmse}\", fontsize = fontsize)\n",
        "            ax = fig.add_subplot(1, 3, 1)\n",
        "            ax.set_xlabel('x', fontsize = fontsize)\n",
        "            ax.set_ylabel('y', fontsize = fontsize)\n",
        "            ax.set_title('$\\sum \\epsilon_i u_i$', fontsize = fontsize)\n",
        "            h = ax.contourf(X, Y, ms, levels = 100, cmap = 'jet', vmin = u_min, vmax = u_max)\n",
        "            cbar = plt.colorbar(h, ax = ax)\n",
        "\n",
        "            ax = fig.add_subplot(1, 3, 2)\n",
        "            ax.set_xlabel('x', fontsize = fontsize)\n",
        "            ax.set_ylabel('y', fontsize = fontsize)\n",
        "            ax.set_title(title, fontsize = fontsize)\n",
        "            h = ax.contourf(X, Y, us, levels = 100, cmap = 'jet', vmin = u_min, vmax = u_max)\n",
        "            cbar = plt.colorbar(h, ax = ax)\n",
        "\n",
        "            ax = fig.add_subplot(1, 3, 3)\n",
        "            ax.set_xlabel('x', fontsize = fontsize)\n",
        "            ax.set_ylabel('y', fontsize = fontsize)\n",
        "            ax.set_title('difference', fontsize = fontsize)\n",
        "            h = ax.contourf(X, Y, us - ms, levels = 100, cmap = 'jet', vmin = err_min, vmax = err_max)\n",
        "            cbar = plt.colorbar(h, ax = ax)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{filename}_total{i}.png\")\n",
        "            print(f\"Saved at {filename}_total{i}.png\")\n",
        "            plt.show()\n",
        "\n",
        "        return plt"
      ],
      "metadata": {
        "id": "COv6NbrPWPOG"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "############################# Plotting ###############################\n",
        "######################################################################\n",
        "\n",
        "\n",
        "mdic = {\"params\": trained_params['net_u'], 'T': T, 'X': X, \"U\": U, \"F\": F, \"loss\": loss_all, \"limit\": limit,\n",
        "        \"scl\": scl, \"epsil\": epsil, \"act_s\": act_s, \"n_iters\": n_iters}\n",
        "FileName = 'Burgers_2d_basic_nu=%.2f.mat' % nu\n",
        "FilePath = str(rootdir.joinpath(FileName))\n",
        "scipy.io.savemat(FilePath, mdic)\n",
        "\n",
        "lb = jnp.array([0, -0.1])  # X_star.min(0)\n",
        "ub = jnp.array([1, 0.1])   # X_star.max(0)\n",
        "limit = [lb, ub]\n",
        "pred_u = sol_pred_create(mdic['limit'], mdic['scl'], epsil = mdic['epsil'], act_s = mdic['act_s'])\n",
        "\n",
        "mdic = scipy.io.loadmat(FilePath)\n",
        "X_star = jnp.hstack((mdic['T'].flatten()[:, None], mdic['X'].flatten()[:, None]))\n",
        "\n",
        "# def neural_net(params, z, limit, scl, act_s):\n",
        "\n",
        "burger_sol = pred_u(X_star)\n",
        "\n",
        "plt.pcolormesh(burger_sol.reshape((201, 101)), cmap = \"jet\")\n",
        "\n",
        "loss_dict = {'loss': loss_all[:, 0], 'loss_d': loss_all[:, 1], 'loss_e': loss_all[:, 2]}\n",
        "\n",
        "plotter = PINN_Plotter(loss_dict = loss_dict, rootdir = rootdir, filename = \"loss.png\", methods = [\"adam\", \"lbfgs\"],\n",
        "                       n_iters = n_iters, dpi = 300, precision = precision, fontsize = {'title': 15, 'subtitle': 12},\n",
        "                       X_star = X_star, pred_u = pred_u)\n",
        "plotter.plot_loss()\n",
        "plotter.plot_1d_ode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "7WZHpEL8MgsJ",
        "outputId": "59d68f31-096a-41cd-89e6-a4c436b4c1da"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/io/matlab/_mio5.py:494: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  narr = np.asanyarray(source)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "sol_pred_create.<locals>.f_u() missing 1 required positional argument: 'z'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-9bb9319f4060>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# def neural_net(params, z, limit, scl, act_s):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mburger_sol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcolormesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mburger_sol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"jet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sol_pred_create.<locals>.f_u() missing 1 required positional argument: 'z'"
          ]
        }
      ]
    }
  ]
}